from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import os

# ×”×’×“×¨×ª ××©×ª× ×™ ×¡×‘×™×‘×”
os.environ['HADOOP_HOME'] = r"C:\hadoop"
os.environ['PATH'] += r";C:\hadoop\bin"

# === ×™×¦×™×¨×ª Spark Session ===
spark = SparkSession.builder \
    .appName("BookRecommenderALS") \
    .master("local[2]") \
    .config("spark.jars", r"C:\spark\jars\sqlite-jdbc-3.45.2.0.jar") \
    .config("spark.driver.extraClassPath", r"C:\spark\jars\sqlite-jdbc-3.45.2.0.jar") \
    .getOrCreate()

print("âœ… Spark Session started successfully")

# === × ×ª×™×‘ ×œ×§×•×‘×¥ DB ===
db_path = r"C:\Users\shira\OneDrive\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\Big Data\final\books\books_no_outliers.db"


# === ×¤×•× ×§×¦×™×” ×œ×˜×¢×™× ×ª ×˜×‘×œ×” ===
def load_table(table_name):
    return spark.read \
        .format("jdbc") \
        .option("url", f"jdbc:sqlite:{db_path}") \
        .option("dbtable", table_name) \
        .option("driver", "org.sqlite.JDBC") \
        .load()


try:
    # === ×˜×¢×™× ×ª ×˜×‘×œ××•×ª ××”-DB ===
    print("ğŸ“š ×˜×•×¢×Ÿ ×˜×‘×œ×ª ×¡×¤×¨×™×...")
    books = load_table("Books")

    print("ğŸ‘¥ ×˜×•×¢×Ÿ ×˜×‘×œ×ª ××©×ª××©×™×...")
    users = load_table("Users")

    print("â­ ×˜×•×¢×Ÿ ×˜×‘×œ×ª ×“×™×¨×•×’×™×...")
    ratings = load_table("Book_Ratings")

    # === ×”×¦×’×ª ××™×“×¢ ×¢×œ ×”×˜×‘×œ××•×ª ===
    print(f"\nğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª:")
    print(f"××¡×¤×¨ ×¡×¤×¨×™×: {books.count()}")
    print(f"××¡×¤×¨ ××©×ª××©×™×: {users.count()}")
    print(f"××¡×¤×¨ ×“×™×¨×•×’×™×: {ratings.count()}")

    # === ×”×¦×’×ª ×“×•×’×××•×ª ××”×“××˜×” ===
    print("\nğŸ“š ×“×•×’×××•×ª ××˜×‘×œ×ª ×¡×¤×¨×™×:")
    books.show(5, truncate=False)

    print("\nğŸ‘¥ ×“×•×’×××•×ª ××˜×‘×œ×ª ××©×ª××©×™×:")
    users.show(5, truncate=False)

    print("\nâ­ ×“×•×’×××•×ª ××˜×‘×œ×ª ×“×™×¨×•×’×™×:")
    ratings.show(5, truncate=False)

    # === ×”×¦×’×ª schema ×©×œ ×”×˜×‘×œ××•×ª ===
    print("\nğŸ“‹ Schema ×©×œ ×˜×‘×œ×ª ×“×™×¨×•×’×™×:")
    ratings.printSchema()

    # === ×¢×™×‘×•×“ ×”×“××˜×” ×œ-ALS ===
    print("\nğŸ”§ ××›×™×Ÿ ×“××˜×” ×¢×‘×•×¨ ALS...")
    ratings_df = ratings.select(
        col("User-ID").cast("long"),
        col("ISBN").cast("string"),
        col("Book-Rating").cast("long")
    ).na.drop()

    print("âœ… ×“××˜×” ××•×›×Ÿ ×¢×‘×•×¨ ALS:")
    ratings_df.show(10)

    print(f"××¡×¤×¨ ×“×™×¨×•×’×™× ×œ××—×¨ × ×™×§×•×™: {ratings_df.count()}")

    # === ×‘×“×™×§×ª ×˜×•×•×— ×”×¢×¨×›×™× ===
    print("\nğŸ“ˆ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×“×™×¨×•×’×™×:")
    ratings_df.describe().show()

except Exception as e:
    print(f"âŒ ×©×’×™××”: {str(e)}")
    print("\nğŸ” ×‘×“×™×§×•×ª ××¤×©×¨×™×•×ª:")
    print("1. ×•×“× ×©×§×•×‘×¥ ×”-DB ×§×™×™× ×‘× ×ª×™×‘ ×©×¦×•×™×Ÿ")
    print("2. ×•×“× ×©×§×•×‘×¥ sqlite-jdbc-3.45.2.0.jar ×§×™×™× ×‘-C:\\spark\\jars\\")
    print("3. ×‘×“×•×§ ×©×”×˜×‘×œ××•×ª books, users, books_ratings ×§×™×™××•×ª ×‘-DB")

finally:
    # × ×™×§×•×™
    spark.stop()
    print("\nğŸ›‘ Spark Session × ×¡×’×¨")